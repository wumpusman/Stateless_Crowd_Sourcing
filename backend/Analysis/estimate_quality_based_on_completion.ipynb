{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "print \"OK\"\n",
    "\n",
    "\"hello\"\n",
    "%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import pandas \n",
    "import pandas as pd \n",
    "import sklearn\n",
    "from backend import db_connection2 as db\n",
    "from backend import db_connection2 \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, meta, session = db.connect(\"postgres\", \"1234\", db=\"match_stick_girl\") #temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_manipulations=session.query(db.Process_Text_Manipulation).all()\n",
    "\n",
    "#baby_born\n",
    "#babay\n",
    "#rewrite_task2\n",
    "\n",
    "#For all Process Rewrite\n",
    "#For all COntent whose origin id == Process_Rewrite\n",
    "#Where User_ID is > -1 #not null\n",
    "\n",
    "#For process - hash_id process and estimated length \n",
    "#For content compeltion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.7272727273\n"
     ]
    }
   ],
   "source": [
    "one_el=all_text_manipulations[0]\n",
    "#one_el=db_connection2.Process_Text_Manipulation\n",
    "#one_el.task_parameters_obj\n",
    "\n",
    "def time_estimate(process):\n",
    "    total_text=\"\"\n",
    "    total_text+=\" \"+process.task_parameters_obj.body_of_task.results\n",
    "    total_text+=\" \"+process.task_parameters_obj.prompt.results\n",
    "    total_text+=\" \"+process.task_parameters_obj.result.results\n",
    "    total_text+=\" \"+process.task_parameters_obj.suggestion.results\n",
    "    total_text+=\" \"+process.task_parameters_obj.context.results\n",
    "    words=len(total_text.split(\" \"))\n",
    "    time = words/ 5.5\n",
    "\n",
    "    return time  \n",
    "\n",
    "print time_estimate(all_text_manipulations[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate rating score\n",
    "#estimate writing score \n",
    "rating_score_matrix=[]\n",
    "writing_score_matrix=[] #writing score matrix first\n",
    "databases=[\"match_stick_girl\",\"baby_born\",\"babay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for database in databases:\n",
    "    conn, meta, session = db.connect(\"postgres\", \"1234\", db=database) \n",
    "    \n",
    "    process=session.query(db.Process_Text_Manipulation).subquery()\n",
    "    content=session.query(db.Content).filter((process.c.id == \n",
    "                                             db.Content.origin_process_id)\n",
    "                                             & (db.Content.is_completed==True)\n",
    "                                             )\n",
    "    content_subquery=content.subquery()\n",
    "    \n",
    "    \n",
    "    task_params=session.query(db.Task_Parameters).filter\\\n",
    "        (content_subquery.c.id==db.Task_Parameters.result_id).subquery()\n",
    "    \n",
    "    process_ratings=session.query(db.Process_Rate).\\\n",
    "        filter(task_params.c.id==db.Process_Rate.task_parameters_id).all()\n",
    "    \n",
    "    #Time it took to read the task, time it took to complete, and overall score\n",
    "    for i in process_ratings:\n",
    "        if  len(i.get_final_results_complete(session).all()) > 0:\n",
    "            result=i.task_parameters_obj.result \n",
    "    \n",
    "            \n",
    "            score= i.get_final_results_complete(session).all()[0].results \n",
    "            \n",
    "            associated_process=session.query(db.Process_Text_Manipulation).filter\\\n",
    "                (result.origin_process_id==db.Process_Text_Manipulation.id).all()[0]\n",
    "            time_to_read= time_estimate(associated_process)\n",
    "            time_to_complete= (result.completed_date-result.assigned_date).total_seconds()\n",
    "            writing_score_matrix.append(\n",
    "                [float(score),time_to_read,time_to_complete])\n",
    "            for j in i.get_content_produced_by_this_process():\n",
    "                if j.is_completed == True:\n",
    "                    time_to_read= time_estimate(i) \n",
    "                    avg_score=float(score)\n",
    "                    score = -2\n",
    "                    try:\n",
    "                        score=float(j.results)\n",
    "                    except: pass \n",
    "                    general_time_to_complete=time_to_complete \n",
    "                    time_to_complete=(j.completed_date-j.assigned_date).total_seconds()\n",
    "                    time_to_read_result=len(result.results.split(\" \"))/5.5\n",
    "                    rating_task=[score,avg_score,time_to_read,time_to_read_result,\n",
    "                                 time_to_complete]\n",
    "                    rating_score_matrix.append(rating_task)\n",
    "        #print \"end of content\""
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "ary=np.array(writing_score_matrix)\n",
    "frame=pd.DataFrame(data=ary,columns=[\"Score\",\"Time_To_Read\",\"Time_To_Complete\"])\n",
    "frame[\"ratio\"]=frame.Time_To_Complete/frame.Time_To_Read\n",
    "function=lambda t: \"Bad\" if t < 3 else \"Good\"\n",
    "frame[\"binary_score\"]=frame['Score'].apply(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "names=preprocessing.LabelEncoder()\n",
    "names.fit(frame['binary_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64367816092\n87\nclasses\n[ 0.  1.]\n87\n87\n86\n          pr        rc       thr\n0   0.627907  1.000000  0.244452\n1   0.623529  0.981481  0.253576\n2   0.630952  0.981481  0.293662\n3   0.626506  0.962963  0.312099\n4   0.621951  0.944444  0.314092\n5   0.617284  0.925926  0.315391\n6   0.625000  0.925926  0.321579\n7   0.620253  0.907407  0.327707\n8   0.628205  0.907407  0.332898\n9   0.623377  0.888889  0.334613\n10  0.618421  0.870370  0.338780\n11  0.626667  0.870370  0.350090\n12  0.621622  0.851852  0.366713\n13  0.616438  0.833333  0.368317\n14  0.611111  0.814815  0.380042\n15  0.605634  0.796296  0.382676\n16  0.600000  0.777778  0.383393\n17  0.594203  0.759259  0.386440\n18  0.588235  0.740741  0.386908\n19  0.582090  0.722222  0.393832\n20  0.575758  0.703704  0.398644\n21  0.584615  0.703704  0.408980\n22  0.578125  0.685185  0.413098\n23  0.587302  0.685185  0.416830\n24  0.580645  0.666667  0.425641\n25  0.573770  0.648148  0.430411\n26  0.566667  0.629630  0.450181\n27  0.559322  0.611111  0.455239\n28  0.568966  0.611111  0.463100\n29  0.561404  0.592593  0.476356\n..       ...       ...       ...\n56  0.633333  0.351852  0.637639\n57  0.655172  0.351852  0.639794\n58  0.678571  0.351852  0.640353\n59  0.703704  0.351852  0.646186\n60  0.692308  0.333333  0.650746\n61  0.680000  0.314815  0.655478\n62  0.666667  0.296296  0.662422\n63  0.695652  0.296296  0.670433\n64  0.727273  0.296296  0.670893\n65  0.714286  0.277778  0.671734\n66  0.700000  0.259259  0.678810\n67  0.736842  0.259259  0.686875\n68  0.777778  0.259259  0.686997\n69  0.823529  0.259259  0.687126\n70  0.812500  0.240741  0.689636\n71  0.866667  0.240741  0.693286\n72  0.857143  0.222222  0.695437\n73  0.923077  0.222222  0.702765\n74  0.916667  0.203704  0.707970\n75  1.000000  0.203704  0.719146\n76  1.000000  0.185185  0.728707\n77  1.000000  0.166667  0.740707\n78  1.000000  0.148148  0.791265\n79  1.000000  0.129630  0.807571\n80  1.000000  0.111111  0.811151\n81  1.000000  0.092593  0.817651\n82  1.000000  0.074074  0.852770\n83  1.000000  0.055556  0.895227\n84  1.000000  0.037037  0.911314\n85  1.000000  0.018519  0.962583\n\n[86 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[25,  8],\n       [23, 31]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn import tree \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#r=tree.DecisionTreeClassifier(criterion=\"entropy\",min_samples_split=10)\n",
    "#r=RandomForestClassifier(min_samples_split=10)\n",
    "r=LogisticRegression()\n",
    "#r=KNeighborsClassifier(n_neighbors=5)\n",
    "p=make_pipeline( r) #PolynomialFeatures(1)\n",
    "\n",
    "#p=LogisticRegression()\n",
    "\n",
    "X=frame[[\"Time_To_Read\",\"Time_To_Complete\",\"ratio\"]]\n",
    "y=frame[\"binary_score\"]\n",
    "X, y = balanced_subsample(X.values,names.transform(y))\n",
    "p.fit(X,y)\n",
    "\n",
    "frame_pos=frame #frame[frame.Score>0] #[frame.binary_score==\"Good\"]\n",
    "X=frame_pos[[\"Time_To_Read\",\"Time_To_Complete\",\"ratio\"]]\n",
    "y=frame_pos[\"binary_score\"]\n",
    "\n",
    "predicted_y= p.predict(X.values)\n",
    "#print y \n",
    "#print predicted_y\n",
    "print p.score(X.values,names.transform(y.values) ) \n",
    "print len(y.values)\n",
    "\n",
    "\n",
    "\n",
    "#print p.coef_\n",
    "#print p.get_params()\n",
    "predicted_prob= p.predict_proba(X.values)\n",
    "#print r.feature_importances_\n",
    "ground_truth_y_predicted_prob=[]\n",
    "print \"classes\"\n",
    "print r.classes_\n",
    "for i,j in enumerate(y):\n",
    "    \n",
    "    location= names.transform([j])[0]\n",
    "    #print \"name\"\n",
    "    #print j \n",
    "    #print location\n",
    "    #print r.classes_\n",
    "    #print predicted_prob[i]\n",
    "    #print predicted_prob[i][location]\n",
    "    ground_truth_y_predicted_prob.append(predicted_prob[i][location])\n",
    "#good set\n",
    "predicted=p.predict(X)\n",
    "#print np.array(ground_truth_y_predicted_prob)\n",
    "prc= precision_recall_curve(names.transform(y.values),\n",
    "                       ground_truth_y_predicted_prob)\n",
    "print len(prc[0])\n",
    "print len(prc[1])\n",
    "print len(prc[2])\n",
    "print pandas.DataFrame(data={\"pr\":prc[0][:-1],\"rc\":prc[1][:-1],\"thr\":prc[2]})\n",
    "confusion_matrix(names.transform(y.values),predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.82845816,  0.17154184]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\"Time_To_Read\",\"Time_To_Complete\",\"ratio\"\n",
    "time_to_read=60\n",
    "time_to_complete=35\n",
    "ratio=time_to_complete/time_to_read\n",
    "p.predict_proba([[time_to_read,time_to_complete,ratio]])\n",
    "\n",
    "#if score > 3\n",
    "# 1- no_prob * score versus\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.545454545455\n[[13  5]\n [15 11]]\n0.581395348837\n[[10  5]\n [13 15]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "k=KFold(n_splits=2)\n",
    "#r=tree.DecisionTreeClassifier(min_samples_split=10)\n",
    "#p=make_pipeline(PolynomialFeatures(1), r)\n",
    "frame_subset=frame #[frame[\"binary_score\"]==\"Bad\"]\n",
    "X=frame_subset[[\"Time_To_Read\",\"Time_To_Complete\",\"ratio\"]]\n",
    "y=frame_subset[[\"binary_score\"]]\n",
    "for train,test in k.split(X.values,y.values):\n",
    "    train_X=X.values[train]\n",
    "    train_y=y.values[train]\n",
    "    train_X,train_y=balanced_subsample(train_X,names.transform(train_y))\n",
    "    test_y=names.transform(y.values[test])\n",
    "    test_X=X.values[test]\n",
    "    p.fit(train_X,train_y)\n",
    "    print p.score(test_X,test_y)\n",
    "    print confusion_matrix(test_y,p.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "ary=np.array(rating_score_matrix)\n",
    "\n",
    "frame=pd.DataFrame(data=ary,columns=[\"Score\",\"Avg_Score\",\"Time_To_Read\",\n",
    "                                     \"Time_To_Read_Result\",\n",
    "                                     \"Time_To_Complete\"])\n",
    "\n",
    "frame[\"ratio\"]=frame.Time_To_Complete/frame.Time_To_Read\n",
    "frame[\"score_diff\"]=(frame.Avg_Score-frame.Score).abs()\n",
    "function=lambda t: \"Bad\" if t > 2 else \"Good\"\n",
    "frame[\"binary_score\"]=frame['score_diff'].apply(function)\n",
    "\n",
    "\n",
    "names=preprocessing.LabelEncoder()\n",
    "names.fit(frame['binary_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import tree\n",
    "\n",
    "r=tree.DecisionTreeClassifier(min_samples_split=10)\n",
    "p=make_pipeline( r) #PolynomialFeatures(1),\n",
    "\n",
    "#p=LogisticRegression()\n",
    "frame=frame #[frame[\"score_diff\"]>0]\n",
    "X=frame[[\"Time_To_Read\",\"Time_To_Read_Result\",\"Time_To_Complete\",\"ratio\"]]\n",
    "y=frame[\"binary_score\"]\n",
    "\n",
    "p.fit(X.values,y.values)\n",
    "\n",
    "frame_pos=frame#[frame.binary_score==\"Bad\"]\n",
    "X=frame_pos[[\"Time_To_Read\",\"Time_To_Read_Result\",\"Time_To_Complete\",\"ratio\"]]\n",
    "y=frame_pos[\"binary_score\"]\n",
    "predicted_y= p.predict(X.values)\n",
    "#print y \n",
    "#print predicted_y\n",
    "#print p.score(X.values,y.values) \n",
    "#print mean_absolute_error(y,predicted_y)\n",
    "#print mean_squared_error(y,predicted_y)\n",
    "#print len(predicted_y)\n",
    "\n",
    "\n",
    "#print pandas.DataFrame(data={\"y_hat\": predicted_y, \"y\":y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0340948214441\n0.14890525501\n0.0915000382271 0.973833735579 1.92607200783\nnext\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "splits=2\n",
    "k=KFold(n_splits=splits,shuffle=True)\n",
    "r=RandomForestRegressor(min_samples_split=5)\n",
    "#r=Ridge()\n",
    "#r=RandomForestRegressor(min_samples_split=5)\n",
    "r=SVR()#KNeighborsRegressor(n_neighbors=10)\n",
    "\n",
    "#r=tree.DecisionTreeClassifier(min_samples_split=10)\n",
    "#p=make_pipeline(PolynomialFeatures(1), r)\n",
    "frame_subset=frame #[frame[\"binary_score\"]==\"Bad\"]\n",
    "X=frame_subset[[\"Score\",\"ratio\",\"Time_To_Read\",\"Time_To_Read_Result\",\"Time_To_Complete\"]]\n",
    "y=frame_subset[\"score_diff\"]\n",
    "\n",
    "X_transformed_scaler=StandardScaler()\n",
    "X_transformed_scaler.fit(X.values)\n",
    "p=make_pipeline(X_transformed_scaler, r)\n",
    "#print X\n",
    "total_score=0\n",
    "total_abs_error=0\n",
    "total_squared_err=0\n",
    "\n",
    "#pandas.DataFrame(data={\"x\":X[\"ratio\"],\"y\":y}).to_csv(\"graph_data.csv\")\n",
    "\n",
    "for train,test in k.split(X.values,y.values):\n",
    "    train_X=X.values[train]\n",
    "    train_y=y.values[train]\n",
    "  \n",
    "    test_y=y.values[test]\n",
    "   \n",
    "    test_X=X.values[test]\n",
    "    \n",
    "    #print len(test_y)\n",
    "    #print len(train_y)\n",
    "    p.fit(train_X,train_y)\n",
    "    #print r.decision_path(test_X)\n",
    "    total_score+= p.score(test_X,test_y)\n",
    "   \n",
    "    predicted_y=p.predict(test_X)\n",
    "    \n",
    "    print p.score(test_X,test_y)\n",
    "   \n",
    "    total_abs_error+= mean_absolute_error(test_y,predicted_y)\n",
    "    total_squared_err+= mean_squared_error(test_y,predicted_y)\n",
    "#print r.feature_importances_\n",
    "print total_score/float(splits),\n",
    "print total_abs_error/float(splits),\n",
    "print total_squared_err/float(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_subsample(x,y,subsample_size=1.0):\n",
    "\n",
    "    class_xs = []\n",
    "    min_elems = None\n",
    "\n",
    "    for yi in np.unique(y):\n",
    "        elems = x[(y == yi)]\n",
    "        class_xs.append((yi, elems))\n",
    "        if min_elems == None or elems.shape[0] < min_elems:\n",
    "            min_elems = elems.shape[0]\n",
    "\n",
    "    use_elems = min_elems\n",
    "    if subsample_size < 1:\n",
    "        use_elems = int(min_elems*subsample_size)\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for ci,this_xs in class_xs:\n",
    "        if len(this_xs) > use_elems:\n",
    "            np.random.shuffle(this_xs)\n",
    "\n",
    "        x_ = this_xs[:use_elems]\n",
    "        y_ = np.empty(use_elems)\n",
    "        y_.fill(ci)\n",
    "\n",
    "        xs.append(x_)\n",
    "        ys.append(y_)\n",
    "\n",
    "    xs = np.concatenate(xs)\n",
    "    ys = np.concatenate(ys)\n",
    "\n",
    "    return xs,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}